{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils.spaces.repeated import Repeated\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from gym.spaces import Discrete\n",
    "from gym.utils import seeding\n",
    "from gym.envs.registration import register\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# choses random numbers from a list\n",
    "# e.g: random.choice([1, 2, 3, 4, 5]) -> 2\n",
    "# returns a random int from an interval:\n",
    "# e.g: random.randint(0, 10) -> 8\n",
    "import random\n",
    "\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.policy.torch_policy_template import build_policy_class\n",
    "import torch\n",
    "\n",
    "def policy_gradient_loss(policy, model, dist_class, train_batch):\n",
    "    logits, _ = model.from_batch(train_batch)\n",
    "    action_dist = dist_class(logits, model)\n",
    "    return -torch.mean(\n",
    "        action_dist.logp(train_batch[\"actions\"]) * train_batch[\"rewards\"])\n",
    "\n",
    "# <class 'ray.rllib.policy.tf_policy_template.MyTFPolicy'>\n",
    "MyTorchPolicy = build_policy_class(\n",
    "                        framework='torch',\n",
    "                        name='MyTorchPolicy',\n",
    "                        loss_fn=policy_gradient_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the distance function:\n",
    "# https://networkx.org/documentation/stable/reference/algorithms/shortest_paths.html\n",
    "\n",
    "class RoadNet(MultiAgentEnv):\n",
    "    def __init__(self, num_as):\n",
    "        self.POS_MIN = 1\n",
    "        self.POS_MAX = 32\n",
    "        self.MAX_STEPS = 15\n",
    "\n",
    "        self.REWARD_AWAY = -2\n",
    "        self.REWARD_STEP = -1\n",
    "        self.REWARD_GOAL = MAX_STEPS\n",
    "        \n",
    "        self.PLATOON = 0\n",
    "        \n",
    "        self.agents = [MockEnv(self.MAX_STEPS)]*num_as\n",
    "        \n",
    "        self.dones = set()\n",
    "        self.obs = {}\n",
    "        self.rew = {}\n",
    "        self.done = {}\n",
    "        self.info = {}\n",
    "        self.num_as = num_as\n",
    "        \n",
    "        self.goal = 25\n",
    "        self.init_positions = list(range(self.POS_MIN, self.POS_MAX)).remove(self.goal)\n",
    "        self.seed()\n",
    "        \n",
    "        self.action_space = Repeated(child_space=Discrete(POS_MAX), \n",
    "                                     max_len=5)\n",
    "        self.observation_space = Discrete(self.POS_MAX + 1)\n",
    "        \n",
    "    def reset(self):\n",
    "        # TODO: \n",
    "        #   - Do not allow agents to start from the same pos.\n",
    "        #   - Make this as dicts per agent.\n",
    "        #   - \n",
    "        \n",
    "        self.dones = set()\n",
    "        self.obs = {}\n",
    "        self.rew = {}\n",
    "        self.done = {}\n",
    "        self.info = {}\n",
    "        \n",
    "        self.position = self.np_random.choice(self.init_positions)\n",
    "        self.count = 0\n",
    "        \n",
    "        self.state = self.position\n",
    "        \n",
    "        \n",
    "        return {i: a.reset() for i, a in enumerate(self.agents)}\n",
    "    \n",
    "    def step(self, action_dict):\n",
    "        obs, rew, done, info = {}, {}, {}, {}\n",
    "        \n",
    "        for i, action in action_dict.items():\n",
    "            obs[i], rew[i], done[i], info[i] = self.agents[i].step(action)\n",
    "            if done[i]:\n",
    "                self.dones.add(i)\n",
    "        done[\"__all__\"] = len(self.dones) == len(self.agents)\n",
    "        \n",
    "        if self.done:\n",
    "            print(\"EPISODE DONE!!!\")\n",
    "        elif self.count == self.MAX_STEPS:\n",
    "            self.done = True\n",
    "        else:\n",
    "            assert self.action_space.contains(action)\n",
    "            self.count += 1\n",
    "            \n",
    "        # ---------------------------------------------\n",
    "        # here define how an action will transition the position\n",
    "        # of the agent from the current state to the next state\n",
    "        # in observation_space.\n",
    "        # also the reward should be included.\n",
    "        # also include the platoon: should compare the states (pos)\n",
    "        # of agents and see if they colide, when true platoon increases\n",
    "        # a reward should be given\n",
    "        \n",
    "        # moving left\n",
    "        if action == self.MOVE_LF:\n",
    "            # if we are at the far left then we lost\n",
    "            # give agent -2\n",
    "            # We can change this by checking the distance between\n",
    "            # current pose and goal if it is more than a diameter\n",
    "            # then -2 should be given else normal move\n",
    "            if self.position == self.LF_MIN:\n",
    "                # invalid\n",
    "                self.reward = self.REWARD_AWAY\n",
    "            else:\n",
    "                # else normal left move\n",
    "                self.position -= 1    \n",
    "                \n",
    "            # if we reached the goal\n",
    "            if self.position == self.goal:\n",
    "                # on goal now\n",
    "                self.reward = self.REWARD_GOAL\n",
    "                self.done = True\n",
    "            elif distance(self.position, self.goal) > self.MAX_STEPS:\n",
    "                # moving away from goal\n",
    "                self.reward = self.REWARD_AWAY\n",
    "            else:\n",
    "                # moving toward goal\n",
    "                self.reward = self.REWARD_STEP\n",
    "        # ---------------------------------------------\n",
    "            \n",
    "        try:\n",
    "            assert self.observation_space.contains(self.state)\n",
    "        except AssertionError:\n",
    "            print(\"INVALID STATE\", self.state)\n",
    "            \n",
    "        # define the other state and info objects\n",
    "        self.state = self.position\n",
    "        self.info[\"dist\"] =  distance(self.position, self.goal)\n",
    "        \n",
    "        return obs, rew, done, info\n",
    "    \n",
    "    def render(self, mode=\"human\"):\n",
    "        # simple print later should render the steps taken by the agents\n",
    "        print(f'position: {self.state} reward: {self.reward:.2f} info: {self.info}')\n",
    "        \n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "register(id=\"roadnet-v0\",\n",
    "         entry_point=\"env:RoadNet\",\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_venv",
   "language": "python",
   "name": "project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
